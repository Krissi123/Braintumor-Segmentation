{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import shutil\n",
    "from PIL import Image \n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nibabel as nib\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "\n",
    "\n",
    "from modules.scandata import MriScan, MriSlice, TumourSegmentation, ScanType, ScanPlane, PatientRecord\n",
    "#from modules.exceptions import ScanDataDirectoryNotFound, ScanFileNotFound\n",
    "\n",
    "# Seed for test/train split and dropping images for undersampling background cases\n",
    "RSEED=78 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScanDataDirectoryNotFound(Exception):\n",
    "    pass\n",
    "class ScanFileNotFound(Exception):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data frame of patients data files\n",
    "raw_data_dir = os.path.join('data', 'UPENN-GBM')\n",
    "raw_scan_data_dir = os.path.join(raw_data_dir, 'images_structural')\n",
    "raw_segmentation_dir = os.path.join(raw_data_dir, 'automated_segm')\n",
    "\n",
    "for dir in raw_scan_data_dir, raw_segmentation_dir:\n",
    "    if not os.path.exists(dir):\n",
    "        raise ScanDataDirectoryNotFound(f'{dir} does not exist')\n",
    "\n",
    "raw_files_list = []\n",
    "\n",
    "# Only work on preop patient data -- identifiers end *_11\n",
    "for patient_scan_dir in glob.glob(os.path.join(raw_scan_data_dir, 'UPENN*11')):\n",
    "    patient_identifier = os.path.basename(patient_scan_dir)\n",
    "    t1_filename = os.path.join(patient_scan_dir, f'{patient_identifier}_T1.nii.gz')\n",
    "    t1ce_filename = os.path.join(patient_scan_dir, f'{patient_identifier}_T1GD.nii.gz')\n",
    "    t2_filename = os.path.join(patient_scan_dir, f'{patient_identifier}_T2.nii.gz')\n",
    "    FLAIR_filename = os.path.join(patient_scan_dir, f'{patient_identifier}_FLAIR.nii.gz')\n",
    "    seg_filename = os.path.join(\n",
    "        raw_segmentation_dir, f'{patient_identifier}_automated_approx_segm.nii.gz'\n",
    "    )\n",
    "\n",
    "    patient_raw_files = [ \n",
    "            patient_identifier, \n",
    "            patient_scan_dir,\n",
    "            t1_filename,\n",
    "            t1ce_filename,\n",
    "            t2_filename,\n",
    "            FLAIR_filename,\n",
    "            seg_filename,\n",
    "    ]\n",
    "    \n",
    "    for file in patient_raw_files[-5:]:\n",
    "        if not os.path.exists(file):\n",
    "            raise ScanFileNotFound(\n",
    "                f'{file} does not exist'\n",
    "            )\n",
    "    raw_files_list.append(patient_raw_files)\n",
    "\n",
    "df_patient_files = pd.DataFrame(raw_files_list, columns=[ \n",
    "            'patient_identifier', \n",
    "            'patient_scan_dir',\n",
    "            't1_filename',\n",
    "            't1ce_filename',\n",
    "            't2_filename',\n",
    "            'FLAIR_filename',\n",
    "            'seg_filename',\n",
    "])   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find all possible slice segmentation types in all patients slices\n",
    "seg_volumes = {\n",
    "    0: 'background',  # either healthy brain tissue or nothing\n",
    "    1: 'tumour', \n",
    "    2: 'edema',\n",
    "    3: 'healthy',\n",
    "    4: 'contrast',\n",
    "}\n",
    "slice_list = []\n",
    "for idx, row in enumerate(df_patient_files.itertuples()):\n",
    "#for seg_filename in df_patient_files.seg_filename:\n",
    "    segmentation = TumourSegmentation(row.seg_filename)\n",
    "    for slice_number , seg_slice in enumerate(segmentation.iterate_slices()):\n",
    "        slice_list.append([\n",
    "            row.patient_identifier,\n",
    "            slice_number,\n",
    "            '_'.join(\n",
    "                [seg_volumes[x] for x in sorted(\n",
    "                    list(set(seg_slice.slice_data.flatten()))\n",
    "                )]\n",
    "            )\n",
    "        ])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_slices = pd.DataFrame(slice_list, columns=['patient_identifier', 'slice_number', 'slice_class'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_slices.slice_class.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clinical = pd.read_csv(\n",
    "    os.path.join(raw_data_dir,'table_data','UPENN-GBM_clinical_info_v1.0.csv')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clinical['age_bin'] = '<40'\n",
    "df_clinical.loc[df_clinical['Age_at_scan_years']>=40, 'age_bin'] = '40-50'\n",
    "df_clinical.loc[df_clinical['Age_at_scan_years']>=50, 'age_bin'] = '50-60'\n",
    "df_clinical.loc[df_clinical['Age_at_scan_years']>=60, 'age_bin'] = '60-70'\n",
    "df_clinical.loc[df_clinical['Age_at_scan_years']>=70, 'age_bin'] = '70-80'\n",
    "df_clinical.loc[df_clinical['Age_at_scan_years']>=80, 'age_bin'] = '>80'\n",
    "\n",
    "df_clinical['stratify_class'] = df_clinical['Gender'] + df_clinical['age_bin']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'stratify_class' in df_patient_files.columns:\n",
    "    df_patient_files.drop('stratify_class', axis=1)\n",
    "df_patient_files = pd.merge(\n",
    "    df_patient_files, \n",
    "    df_clinical[['ID','stratify_class']], \n",
    "    how='left', \n",
    "    left_on='patient_identifier', \n",
    "    right_on='ID'\n",
    ").drop('ID', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Allocate patient data to train or test set\n",
    "patients = df_patient_files[['patient_identifier', 'stratify_class']]\n",
    "train_patients, test_patients = train_test_split(\n",
    "    patients, \n",
    "    test_size=0.1, \n",
    "    random_state=RSEED,\n",
    "    stratify=df_patient_files.stratify_class\n",
    ")\n",
    "df_train_patients = pd.DataFrame(train_patients)\n",
    "df_test_patients = pd.DataFrame(test_patients)\n",
    "df_train_patients['test_train_set'] = 'train'\n",
    "df_test_patients['test_train_set'] = 'test'\n",
    "\n",
    "if 'test_train_set' in df_slices.columns:\n",
    "    df_slices = df_slices.drop('test_train_set', axis=1)\n",
    "if 'stratify_class' in df_slices.columns:\n",
    "    df_slices = df_slices.drop('stratify_class', axis=1)\n",
    "df_slices = df_slices.merge(\n",
    "    pd.concat([df_test_patients, df_train_patients]), \n",
    "    on='patient_identifier', \n",
    "    how='left',\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create classification directories\n",
    "classification_data_dir = os.path.join(raw_data_dir, 'slice_classification_common_stratify_healthysegment')\n",
    "try:\n",
    "    os.mkdir(classification_data_dir)\n",
    "except:\n",
    "    print(f'{classification_data_dir} already exists')\n",
    "\n",
    "# Split into test and train\n",
    "for data_set in 'test', 'train':\n",
    "    data_set_dir = os.path.join(classification_data_dir, data_set)\n",
    "    try:\n",
    "        os.mkdir(data_set_dir)\n",
    "    except:\n",
    "        print(f'{data_set_dir} already exists')\n",
    "\n",
    "    for slice_class in df_slices.slice_class.unique():\n",
    "        class_dir = os.path.join(data_set_dir, slice_class)\n",
    "        try:\n",
    "            os.mkdir(class_dir)\n",
    "        except:\n",
    "            print(f'{class_dir} already exists')\n",
    "            \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, row in enumerate(df_patient_files.itertuples()):\n",
    "    T1_scan = MriScan(\n",
    "        filename=row.t1_filename,\n",
    "        sequence=ScanType.T1\n",
    "    )\n",
    "    T1CE_scan = MriScan(\n",
    "        filename=row.t1ce_filename,\n",
    "        sequence=ScanType.T1CE\n",
    "    )\n",
    "    T2_scan = MriScan(\n",
    "        filename=row.t2_filename,\n",
    "        sequence=ScanType.T2\n",
    "    )\n",
    "    FLAIR_scan = MriScan(\n",
    "        filename=row.FLAIR_filename,\n",
    "        sequence=ScanType.FLAIR\n",
    "    )\n",
    "    segmentation = TumourSegmentation(\n",
    "        row.seg_filename,\n",
    "        )\n",
    "    patient_data = PatientRecord()\n",
    "    patient_data.add_scan_data(T1_scan)\n",
    "    patient_data.add_scan_data(T1CE_scan)\n",
    "    patient_data.add_scan_data(T2_scan)\n",
    "    patient_data.add_scan_data(FLAIR_scan)\n",
    "    patient_data.add_segmentation(segmentation)\n",
    "    patient_data.segment_healthy_tissue()\n",
    "    png_basename_prefix = f'{row.patient_identifier}_allseq'\n",
    "    patient_data.save_multi_channel_png(os.path.join(classification_data_dir,png_basename_prefix))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_slice_class_counts = df_slices.query(\n",
    "    'test_train_set == \"train\"'\n",
    ").slice_class.value_counts().to_frame().reset_index().set_axis(\n",
    "    ['slice_class', 'slice_class_count'], axis=1\n",
    ")\n",
    "df_slices =  pd.merge(df_slices,df_slice_class_counts, how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate class weight due to unbalanced classes\n",
    "print(df_slices.query('test_train_set == \"train\"').slice_class.value_counts())\n",
    "# Background only slices most common and least useful for training. \n",
    "# Take only 30 % in training sample data set and drop classes with less than 100 slices\n",
    "df_sliced_dropped_background =  df_slices.query(\n",
    "        'test_train_set == \"train\" and slice_class == \"background\"'\n",
    ").sample(frac=0.70, random_state=RSEED)\n",
    "df_sliced_dropped_rare = df_slices.query('slice_class_count < 100')\n",
    "df_slices_after_drop = df_slices.drop(df_sliced_dropped_background.index)\n",
    "df_slices_after_drop = df_slices_after_drop.drop(df_sliced_dropped_rare.index)\n",
    "print(\"After Dropping:\")\n",
    "print(df_slices_after_drop.query('test_train_set == \"train\"' ).slice_class.value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_weight = compute_class_weight(\n",
    "    class_weight='balanced',\n",
    "    classes = np.unique(df_slices_after_drop['slice_class']),\n",
    "    y = df_slices_after_drop['slice_class']\n",
    ")\n",
    "class_weight_dict = dict(\n",
    "    zip(np.unique(df_slices_after_drop['slice_class']),class_weight)\n",
    ")\n",
    "print(class_weight_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move data to classification directories\n",
    "for idx, row in enumerate(df_slices_after_drop.itertuples()):\n",
    "    src_file = os.path.join(\n",
    "        classification_data_dir,\n",
    "        f'{row.patient_identifier}_allseq_{row.slice_number:03}.png'\n",
    "    )\n",
    "    dest_dir = os.path.join(classification_data_dir,row.test_train_set,row.slice_class)\n",
    "    shutil.move(src_file,dest_dir)\n",
    "# Delete dropped  files to clean up\n",
    "for frame in df_sliced_dropped_background, df_sliced_dropped_rare:\n",
    "    for idx, row in enumerate(frame.itertuples()):\n",
    "        del_file = os.path.join(\n",
    "            classification_data_dir,\n",
    "            f'{row.patient_identifier}_allseq_{row.slice_number:03}.png'\n",
    "        )\n",
    "        os.remove(del_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Delete unused class directories\n",
    "for data_set in 'test', 'train':\n",
    "    data_set_dir = os.path.join(classification_data_dir, data_set)\n",
    "    for slice_class in df_sliced_dropped_rare.slice_class.unique():\n",
    "        \n",
    "        rm_dir = os.path.join(data_set_dir,slice_class)\n",
    "        try:\n",
    "            os.rmdir(rm_dir)\n",
    "        except:\n",
    "            print(f'{rm_dir} does not exist')\n",
    "         \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.merge(\n",
    "    df_slices.query('test_train_set == \"train\"').stratify_class.value_counts().reset_index().set_axis(['class', 'ntrain'], axis=1),\n",
    "    df_slices.query('test_train_set == \"test\"').stratify_class.value_counts().reset_index().set_axis(['class', 'ntest'], axis=1),\n",
    "    on='class'\n",
    ").eval('frac = ntrain/ntest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_slices"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
