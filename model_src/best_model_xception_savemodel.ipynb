{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "import mlflow\n",
    "from dotenv import load_dotenv\n",
    "import tensorflow as tf\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import classification_report\n",
    "from tensorflow import keras \n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "load_dotenv('../.env')\n",
    "\n",
    "RSEED = 123\n",
    "MODELS_DIR=os.path.join('..','models')\n",
    "MODEL_CHECKPOINTS_DIR=os.path.join('..','model_checkpoints')\n",
    "\n",
    "start_time = datetime.now().strftime('-%Y-%m-%d-%T')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dir in MODELS_DIR, MODEL_CHECKPOINTS_DIR:\n",
    "    if not os.path.exists(dir):\n",
    "        os.mkdir(dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set parameters\n",
    "batch_size=64\n",
    "patience=10 \n",
    "min_delta=0.001\n",
    "dropout_rate=0.25\n",
    "initial_learning_rate=0.0005\n",
    "\n",
    "\n",
    "run_name_params = (\n",
    "    f'bs{batch_size}'\n",
    "    f'_pat{patience}'\n",
    "    f'_del{min_delta}'\n",
    "    f'_dr{dropout_rate}'\n",
    "    f'_lr{initial_learning_rate}'\n",
    ")\n",
    "\n",
    "parent_run_name = f'mobilenetv2_{run_name_params}_save'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up gcloud TPUs\n",
    "cluster_resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu='local')\n",
    "tf.config.experimental_connect_to_cluster(cluster_resolver)\n",
    "tf.tpu.experimental.initialize_tpu_system(cluster_resolver)\n",
    "strategy = tf.distribute.TPUStrategy(cluster_resolver)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set information for mlflow\n",
    "run_description = \"\"\"\n",
    "Fully trained models \n",
    "    - classify each slice by tumour/tissue regions in the segmentation\n",
    "    - Uses MobileNetV2\n",
    "    - Saves model at end of run\n",
    "\"\"\"\n",
    "dataset = 'full_data_stratified'\n",
    "mlflow_tracking_uri = os.getenv('MLFLOW_URI')\n",
    "if mlflow_tracking_uri:\n",
    "    mlflow.set_tracking_uri(mlflow_tracking_uri)\n",
    "mlflow_expt = os.getenv('CLASSIFICATION_EXPT')\n",
    "if mlflow_expt:\n",
    "    mlflow.set_experiment(mlflow_expt)    \n",
    "\n",
    "\n",
    "print(f'Logging to \\n URI:{mlflow_tracking_uri}\\n Expt:{mlflow_expt}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "with mlflow.start_run(\n",
    "    run_name=parent_run_name,\n",
    "    tags={\n",
    "        'dataset': dataset,\n",
    "    },\n",
    "    description=run_description,\n",
    "):\n",
    "\n",
    "    img_height = 240\n",
    "    img_width = 240\n",
    "    data_dir = os.path.join('..','data','UPENN-GBM','slice_classification_common_stratify','train')\n",
    "\n",
    "    train_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "        data_dir,\n",
    "        validation_split=0.2,\n",
    "        subset=\"training\",\n",
    "        color_mode=\"rgba\",\n",
    "        seed=RSEED,\n",
    "        image_size=(img_height, img_width),\n",
    "        batch_size=batch_size)\n",
    "\n",
    "    val_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "        data_dir,\n",
    "        validation_split=0.2,\n",
    "        subset=\"validation\",\n",
    "        color_mode=\"rgba\",\n",
    "        seed=RSEED,\n",
    "        image_size=(img_height, img_width),\n",
    "        batch_size=batch_size)\n",
    "\n",
    "    \n",
    "    class_names = train_ds.class_names\n",
    "\n",
    "    # Calculate class weights for weighting accuracy\n",
    "    ds_classes = []\n",
    "    for _, batch_classes in train_ds:\n",
    "        ds_classes.append(batch_classes.numpy())\n",
    "\n",
    "    ds_classes = np.concatenate(ds_classes)\n",
    "\n",
    "    class_weight = compute_class_weight(\n",
    "        class_weight = 'balanced',\n",
    "        classes = np.unique(ds_classes),\n",
    "        y=ds_classes\n",
    "    )\n",
    "\n",
    "    class_weight = dict(zip(np.unique(ds_classes), class_weight))\n",
    "\n",
    "    AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "    train_ds = train_ds.cache().shuffle(1000).prefetch(buffer_size=AUTOTUNE)\n",
    "    val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "\n",
    "    num_classes = len(class_names)\n",
    "    \n",
    "    margin = 8\n",
    "    scaled_height = img_height - 2*margin\n",
    "    scaled_width = img_width - 2*margin\n",
    "\n",
    "    # Build layers for model with fixed base\n",
    "    with strategy.scope():\n",
    "        crop_layer = tf.keras.layers.Cropping2D(margin)\n",
    "        rescale_initial = tf.keras.layers.Rescaling(1./127.5, offset=-1)\n",
    "        conv_4to3_channel = tf.keras.layers.Conv2D(3,1,padding='same',activation='tanh')\n",
    "        base_model = tf.keras.applications.MobileNetV2(\n",
    "            input_shape=(scaled_width,scaled_height,3),\n",
    "            include_top=False,\n",
    "            weights='imagenet'\n",
    "        )\n",
    "        global_average_layer = tf.keras.layers.GlobalAveragePooling2D()\n",
    "        prediction_layer = tf.keras.layers.Dense(num_classes)\n",
    "\n",
    "        base_model.trainable = False\n",
    "        \n",
    "        inputs = tf.keras.Input(shape=(img_width, img_height, 4))\n",
    "        x = crop_layer(inputs)\n",
    "        x = rescale_initial(x)\n",
    "        x = conv_4to3_channel(x)\n",
    "        x = base_model(x, training=False)\n",
    "        x = global_average_layer(x)\n",
    "        x = tf.keras.layers.Dropout(dropout_rate)(x)\n",
    "        outputs = prediction_layer(x)\n",
    "    \n",
    "        earlystopping = tf.keras.callbacks.EarlyStopping(\n",
    "            monitor='val_loss',\n",
    "            patience=patience,\n",
    "            min_delta=min_delta,\n",
    "            )\n",
    "        \n",
    "        model = tf.keras.Model(inputs, outputs)\n",
    "        model.compile(\n",
    "            optimizer=tf.keras.optimizers.Adam(learning_rate=initial_learning_rate,),\n",
    "            loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "            metrics=['accuracy'],\n",
    "        )\n",
    "\n",
    "        \n",
    "    # Initial fit of classification and 4 to 3 channel layers\n",
    "    with mlflow.start_run(\n",
    "        run_name=f'fixed_{run_name_params}',\n",
    "        tags={'dataset': dataset},\n",
    "        nested=True\n",
    "    ):\n",
    "        mlflow.tensorflow.autolog()\n",
    "        mlflow.log_param('ds_batch_size', batch_size)\n",
    "        mlflow.log_param('ds_validation_batch_size', batch_size)\n",
    "\n",
    "\n",
    "        fixed_base_epochs=80\n",
    "        history_fixed_base = model.fit(\n",
    "            train_ds,\n",
    "            validation_data=val_ds,\n",
    "            epochs=fixed_base_epochs,\n",
    "            class_weight=class_weight,\n",
    "            callbacks=[earlystopping],\n",
    "        )\n",
    "\n",
    "    # Relax top layers of base model\n",
    "    base_model.trainable = True\n",
    "    fix_below_layer = 100\n",
    "    for layer in base_model.layers[:fix_below_layer]:\n",
    "        layer.trainable = False\n",
    "    with strategy.scope():\n",
    "        model.compile(\n",
    "            optimizer=tf.keras.optimizers.Adam(learning_rate=initial_learning_rate/10.0),\n",
    "                loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "                metrics=['accuracy'],\n",
    "        )\n",
    "\n",
    "    with mlflow.start_run(\n",
    "        run_name=f'partial_{run_name_params}',\n",
    "        tags={'dataset': dataset},\n",
    "        nested=True\n",
    "    ):\n",
    "        mlflow.tensorflow.autolog()\n",
    "        mlflow.log_param('ds_batch_size', batch_size)\n",
    "        mlflow.log_param('ds_validation_batch_size', batch_size)\n",
    "\n",
    "        partial_relax_epochs=history_fixed_base.epoch[-1] + 100 \n",
    "        history_partial_relax = model.fit(\n",
    "            train_ds,\n",
    "            validation_data=val_ds,\n",
    "            epochs=partial_relax_epochs,\n",
    "            initial_epoch=history_fixed_base.epoch[-1]+1,\n",
    "            class_weight=class_weight,\n",
    "            callbacks=[earlystopping],\n",
    "        )\n",
    "\n",
    "    # Fully relax model\n",
    "    model.trainable = True\n",
    "\n",
    "    with strategy.scope():\n",
    "        model.compile(\n",
    "            optimizer=tf.keras.optimizers.Adam(learning_rate=initial_learning_rate/10.0),\n",
    "            loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "            metrics=['accuracy'],\n",
    "        )\n",
    "\n",
    "    with mlflow.start_run(\n",
    "        run_name=f'relax_{run_name_params}',\n",
    "        tags={'dataset': dataset},\n",
    "        nested=True\n",
    "    ):\n",
    "        mlflow.tensorflow.autolog()\n",
    "        mlflow.log_param('ds_batch_size', batch_size)\n",
    "        mlflow.log_param('ds_validation_batch_size', batch_size)\n",
    "\n",
    "        # create checkpoint\n",
    "        checkpoint_path = os.path.join(\n",
    "            MODEL_CHECKPOINTS_DIR,\n",
    "            parent_run_name + start_time + \"-{epoch:03d}-{val_loss:.4f}.ckpt\"\n",
    "        )\n",
    "        ckpt_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "            filepath=checkpoint_path, \n",
    "            verbose=1, \n",
    "            save_weights_only=False,\n",
    "            save_freq='epoch',\n",
    "            monitor='val_loss',\n",
    "            mode='min',\n",
    "            save_best_only=True,\n",
    "        ) \n",
    "\n",
    "        full_relax_epochs=history_partial_relax.epoch[-1] + 100\n",
    "        history_full_relax = model.fit(\n",
    "            train_ds,\n",
    "            validation_data=val_ds,\n",
    "            epochs=full_relax_epochs,\n",
    "            initial_epoch=history_partial_relax.epoch[-1]+1,\n",
    "            class_weight=class_weight,\n",
    "            callbacks=[earlystopping, ckpt_callback],\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_dir = os.path.join('..','data','UPENN-GBM','slice_classification_common_stratify','test')\n",
    "\n",
    "test_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "    test_data_dir,\n",
    "    color_mode=\"rgba\",\n",
    "    seed=RSEED,\n",
    "    shuffle=False,\n",
    "    image_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_pred = model.predict(val_ds)\n",
    "val_prob = tf.nn.softmax(val_pred)\n",
    "val_class_pred = [np.argmax(x) for x in val_prob]\n",
    "val_base = [ 0 for x in val_class_pred ]\n",
    "\n",
    "val_true_class = []\n",
    "for _, classes in val_ds:\n",
    "    val_true_class += list(classes)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred = model.predict(test_ds)\n",
    "test_prob = tf.nn.softmax(test_pred)\n",
    "test_class_pred = [np.argmax(x) for x in test_prob]\n",
    "test_base = [ 0 for x in test_class_pred ]\n",
    "\n",
    "test_true_class = []\n",
    "for _, classes in test_ds:\n",
    "    test_true_class += list(classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(val_true_class, val_class_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(test_true_class, test_class_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_file_name = os.path.join(MODELS_DIR, parent_run_name + start_time)\n",
    "model.save(model_file_name)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
