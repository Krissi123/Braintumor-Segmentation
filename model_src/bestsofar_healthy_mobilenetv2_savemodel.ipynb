{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-02 09:29:13.315799: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-12-02 09:29:13.507610: I tensorflow/core/tpu/tpu_initializer_helper.cc:262] Libtpu path is: libtpu.so\n",
      "I1202 09:29:13.609705268  215361 ev_epoll1_linux.cc:121]     grpc epoll fd: 69\n",
      "D1202 09:29:13.609728099  215361 ev_posix.cc:141]            Using polling engine: epoll1\n",
      "D1202 09:29:13.609825542  215361 lb_policy_registry.cc:43]   registering LB policy factory for \"grpclb\"\n",
      "D1202 09:29:13.609836896  215361 lb_policy_registry.cc:43]   registering LB policy factory for \"rls_experimental\"\n",
      "D1202 09:29:13.609847234  215361 lb_policy_registry.cc:43]   registering LB policy factory for \"priority_experimental\"\n",
      "D1202 09:29:13.609850613  215361 lb_policy_registry.cc:43]   registering LB policy factory for \"weighted_target_experimental\"\n",
      "D1202 09:29:13.609853742  215361 lb_policy_registry.cc:43]   registering LB policy factory for \"pick_first\"\n",
      "D1202 09:29:13.609856759  215361 lb_policy_registry.cc:43]   registering LB policy factory for \"round_robin\"\n",
      "D1202 09:29:13.609865713  215361 lb_policy_registry.cc:43]   registering LB policy factory for \"ring_hash_experimental\"\n",
      "D1202 09:29:13.609876732  215361 dns_resolver_ares.cc:831]   Using ares dns resolver\n",
      "D1202 09:29:13.609913884  215361 certificate_provider_registry.cc:39] registering certificate provider factory for \"file_watcher\"\n",
      "D1202 09:29:13.609922908  215361 lb_policy_registry.cc:43]   registering LB policy factory for \"cds_experimental\"\n",
      "D1202 09:29:13.609926590  215361 lb_policy_registry.cc:43]   registering LB policy factory for \"xds_cluster_impl_experimental\"\n",
      "D1202 09:29:13.609934960  215361 lb_policy_registry.cc:43]   registering LB policy factory for \"xds_cluster_resolver_experimental\"\n",
      "D1202 09:29:13.609938306  215361 lb_policy_registry.cc:43]   registering LB policy factory for \"xds_cluster_manager_experimental\"\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "import mlflow\n",
    "from dotenv import load_dotenv\n",
    "import tensorflow as tf\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import classification_report\n",
    "from tensorflow import keras \n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "load_dotenv('../.env')\n",
    "\n",
    "RSEED = 123\n",
    "MODELS_DIR=os.path.join('..','models')\n",
    "MODEL_CHECKPOINTS_DIR=os.path.join('..','model_checkpoints')\n",
    "\n",
    "start_time = datetime.now().strftime('-%Y-%m-%d-%T')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dir in MODELS_DIR, MODEL_CHECKPOINTS_DIR:\n",
    "    if not os.path.exists(dir):\n",
    "        os.mkdir(dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set parameters\n",
    "batch_size=16\n",
    "patience=5\n",
    "min_delta=0.001\n",
    "dropout_rate=0.25\n",
    "initial_learning_rate=0.0001\n",
    "\n",
    "\n",
    "run_name_params = (\n",
    "    f'bs{batch_size}'\n",
    "    f'_pat{patience}'\n",
    "    f'_del{min_delta}'\n",
    "    f'_dr{dropout_rate}'\n",
    "    f'_lr{initial_learning_rate}'\n",
    ")\n",
    "\n",
    "parent_run_name = f'mobilenetv2_{run_name_params}_save'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Deallocate tpu buffers before initializing tpu system.\n",
      "INFO:tensorflow:Initializing the TPU system: local\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-02 09:29:16.338207: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-12-02 09:29:30.202506: I tensorflow/compiler/xla/service/service.cc:173] XLA service 0x87b31e0 initialized for platform TPU (this does not guarantee that XLA will be used). Devices:\n",
      "2022-12-02 09:29:30.202544: I tensorflow/compiler/xla/service/service.cc:181]   StreamExecutor device (0): TPU, 2a886c8\n",
      "2022-12-02 09:29:30.202552: I tensorflow/compiler/xla/service/service.cc:181]   StreamExecutor device (1): TPU, 2a886c8\n",
      "2022-12-02 09:29:30.202558: I tensorflow/compiler/xla/service/service.cc:181]   StreamExecutor device (2): TPU, 2a886c8\n",
      "2022-12-02 09:29:30.202564: I tensorflow/compiler/xla/service/service.cc:181]   StreamExecutor device (3): TPU, 2a886c8\n",
      "2022-12-02 09:29:30.202570: I tensorflow/compiler/xla/service/service.cc:181]   StreamExecutor device (4): TPU, 2a886c8\n",
      "2022-12-02 09:29:30.202576: I tensorflow/compiler/xla/service/service.cc:181]   StreamExecutor device (5): TPU, 2a886c8\n",
      "2022-12-02 09:29:30.202582: I tensorflow/compiler/xla/service/service.cc:181]   StreamExecutor device (6): TPU, 2a886c8\n",
      "2022-12-02 09:29:30.202588: I tensorflow/compiler/xla/service/service.cc:181]   StreamExecutor device (7): TPU, 2a886c8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Finished initializing TPU system.\n",
      "INFO:tensorflow:Found TPU system:\n",
      "INFO:tensorflow:*** Num TPU Cores: 8\n",
      "INFO:tensorflow:*** Num TPU Workers: 1\n",
      "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n"
     ]
    }
   ],
   "source": [
    "# Set up gcloud TPUs\n",
    "cluster_resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu='local')\n",
    "tf.config.experimental_connect_to_cluster(cluster_resolver)\n",
    "tf.tpu.experimental.initialize_tpu_system(cluster_resolver)\n",
    "strategy = tf.distribute.TPUStrategy(cluster_resolver)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging to \n",
      " URI:https://hudsju377cddpoevnjdkfnvpwovniewnipcdsnkvn.mlflow.neuefische.de\n",
      " Expt:braintumour_mri_slice_classification\n"
     ]
    }
   ],
   "source": [
    "# Set information for mlflow\n",
    "run_description = \"\"\"\n",
    "Fully trained models \n",
    "    - classify each slice by tumour/tissue regions in the segmentation\n",
    "    - Uses MobileNetV2\n",
    "    - Saves model at end of run\n",
    "\"\"\"\n",
    "dataset = 'slice_classification_common_stratify_healthysegmented'\n",
    "mlflow_tracking_uri = os.getenv('MLFLOW_URI')\n",
    "if mlflow_tracking_uri:\n",
    "    mlflow.set_tracking_uri(mlflow_tracking_uri)\n",
    "mlflow_expt = os.getenv('CLASSIFICATION_EXPT')\n",
    "if mlflow_expt:\n",
    "    mlflow.set_experiment(mlflow_expt)    \n",
    "\n",
    "\n",
    "print(f'Logging to \\n URI:{mlflow_tracking_uri}\\n Expt:{mlflow_expt}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 79212 files belonging to 6 classes.\n",
      "Using 63370 files for training.\n",
      "Found 79212 files belonging to 6 classes.\n",
      "Using 15842 files for validation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-02 09:29:53.272092: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2022-12-02 09:29:53.340482: I tensorflow/compiler/jit/xla_compilation_cache.cc:476] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2022-12-02 09:30:13.989819: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Found an unshardable source dataset: name: \"TensorSliceDataset/_1\"\n",
      "op: \"TensorSliceDataset\"\n",
      "input: \"Placeholder/_0\"\n",
      "attr {\n",
      "  key: \"Toutput_types\"\n",
      "  value {\n",
      "    list {\n",
      "      type: DT_STRING\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"_cardinality\"\n",
      "  value {\n",
      "    i: 63370\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"is_files\"\n",
      "  value {\n",
      "    b: false\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"metadata\"\n",
      "  value {\n",
      "    s: \"\\n\\024TensorSliceDataset:0\"\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_shapes\"\n",
      "  value {\n",
      "    list {\n",
      "      shape {\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"replicate_on_split\"\n",
      "  value {\n",
      "    b: false\n",
      "  }\n",
      "}\n",
      "experimental_type {\n",
      "  type_id: TFT_PRODUCT\n",
      "  args {\n",
      "    type_id: TFT_DATASET\n",
      "    args {\n",
      "      type_id: TFT_PRODUCT\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_STRING\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n",
      "2022/12/02 09:30:15 WARNING mlflow.utils.autologging_utils: Encountered unexpected error during autologging: Unable to locate credentials\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-02 09:30:22.175143: I tensorflow/core/tpu/graph_rewrite/encapsulate_tpu_computations_pass.cc:237] Subgraph fingerprint:11463474533832690926\n",
      "2022-12-02 09:30:22.467672: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:954] model_pruner failed: INVALID_ARGUMENT: Graph does not contain terminal node Adam/Adam/AssignAddVariableOp.\n",
      "2022-12-02 09:30:22.772155: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:954] model_pruner failed: INVALID_ARGUMENT: Graph does not contain terminal node Adam/Adam/AssignAddVariableOp.\n",
      "2022-12-02 09:30:25.479511: I tensorflow/core/tpu/kernels/tpu_compilation_cache_interface.cc:437] TPU host compilation cache miss: cache_key(12182687663336830996), session_name()\n",
      "2022-12-02 09:30:31.561300: I tensorflow/core/tpu/kernels/tpu_compile_op_common.cc:210] Compilation of 12182687663336830996 with session name  took 6.081654767s and succeeded\n",
      "2022-12-02 09:30:31.598083: I tensorflow/core/tpu/kernels/tpu_compilation_cache_interface.cc:470] TPU host compilation cache: compilation complete for cache_key(12182687663336830996), session_name(), subgraph_key(std::string(property.function_name) = \"cluster_train_function_11463474533832690926\", property.function_library_fingerprint = 6329225123068742150, property.mlir_module_fingerprint = 0, property.num_replicas = 8, topology.chip_bounds().x = 2, topology.chip_bounds().y = 2, topology.chip_bounds().z = 1, topology.wrap().x = false, topology.wrap().y = false, topology.wrap().z = false, topology.MissingChipCount() = 0, std::string(property.shapes_prefix) = \"2,240,240,4,;2,;2,;\", property.guaranteed_constants_size = 0, embedding_partitions_fingerprint = \"1688352644216761960\")\n",
      "2022-12-02 09:30:31.598145: I tensorflow/core/tpu/kernels/tpu_compilation_cache_interface.cc:536] After adding entry for key 12182687663336830996 with session_name  cache is 1 entries (27416351 bytes),  marked for eviction 0 entries (0 bytes).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   5/3961 [..............................] - ETA: 2:15 - loss: 1.8704 - accuracy: 0.0375  WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0283s vs `on_train_batch_end` time: 0.0359s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0283s vs `on_train_batch_end` time: 0.0359s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3959/3961 [============================>.] - ETA: 0s - loss: 1.2878 - accuracy: 0.4761"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-02 09:32:12.099721: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Found an unshardable source dataset: name: \"TensorSliceDataset/_1\"\n",
      "op: \"TensorSliceDataset\"\n",
      "input: \"Placeholder/_0\"\n",
      "attr {\n",
      "  key: \"Toutput_types\"\n",
      "  value {\n",
      "    list {\n",
      "      type: DT_STRING\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"_cardinality\"\n",
      "  value {\n",
      "    i: 15842\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"is_files\"\n",
      "  value {\n",
      "    b: false\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"metadata\"\n",
      "  value {\n",
      "    s: \"\\n\\024TensorSliceDataset:7\"\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_shapes\"\n",
      "  value {\n",
      "    list {\n",
      "      shape {\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"replicate_on_split\"\n",
      "  value {\n",
      "    b: false\n",
      "  }\n",
      "}\n",
      "experimental_type {\n",
      "  type_id: TFT_PRODUCT\n",
      "  args {\n",
      "    type_id: TFT_DATASET\n",
      "    args {\n",
      "      type_id: TFT_PRODUCT\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_STRING\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n",
      "2022-12-02 09:32:14.561614: I tensorflow/core/tpu/graph_rewrite/encapsulate_tpu_computations_pass.cc:237] Subgraph fingerprint:7162480873429290547\n",
      "2022-12-02 09:32:14.709278: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:954] model_pruner failed: INVALID_ARGUMENT: Graph does not contain terminal node AssignAddVariableOp.\n",
      "2022-12-02 09:32:14.926494: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:954] model_pruner failed: INVALID_ARGUMENT: Graph does not contain terminal node AssignAddVariableOp.\n",
      "2022-12-02 09:32:15.331744: I tensorflow/core/tpu/kernels/tpu_compilation_cache_interface.cc:437] TPU host compilation cache miss: cache_key(8425954318022435198), session_name()\n",
      "2022-12-02 09:32:18.868049: I tensorflow/core/tpu/kernels/tpu_compile_op_common.cc:210] Compilation of 8425954318022435198 with session name  took 3.5362125s and succeeded\n",
      "2022-12-02 09:32:18.881362: I tensorflow/core/tpu/kernels/tpu_compilation_cache_interface.cc:470] TPU host compilation cache: compilation complete for cache_key(8425954318022435198), session_name(), subgraph_key(std::string(property.function_name) = \"cluster_test_function_7162480873429290547\", property.function_library_fingerprint = 1002640937815527725, property.mlir_module_fingerprint = 0, property.num_replicas = 8, topology.chip_bounds().x = 2, topology.chip_bounds().y = 2, topology.chip_bounds().z = 1, topology.wrap().x = false, topology.wrap().y = false, topology.wrap().z = false, topology.MissingChipCount() = 0, std::string(property.shapes_prefix) = \"2,240,240,4,;2,;\", property.guaranteed_constants_size = 0, embedding_partitions_fingerprint = \"1688352644216761960\")\n",
      "2022-12-02 09:32:18.881426: I tensorflow/core/tpu/kernels/tpu_compilation_cache_interface.cc:536] After adding entry for key 8425954318022435198 with session_name  cache is 2 entries (40788503 bytes),  marked for eviction 0 entries (0 bytes).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3961/3961 [==============================] - 139s 31ms/step - loss: 1.2876 - accuracy: 0.4761 - val_loss: 1.1645 - val_accuracy: 0.5309\n",
      "Epoch 2/80\n",
      "3961/3961 [==============================] - 113s 29ms/step - loss: 1.1168 - accuracy: 0.5641 - val_loss: 1.0557 - val_accuracy: 0.5965\n",
      "Epoch 3/80\n",
      "3961/3961 [==============================] - 113s 28ms/step - loss: 1.0381 - accuracy: 0.5908 - val_loss: 0.9288 - val_accuracy: 0.6578\n",
      "Epoch 4/80\n",
      "3961/3961 [==============================] - 112s 28ms/step - loss: 0.9941 - accuracy: 0.6100 - val_loss: 0.9513 - val_accuracy: 0.6369\n",
      "Epoch 5/80\n",
      "3961/3961 [==============================] - 111s 28ms/step - loss: 0.9529 - accuracy: 0.6265 - val_loss: 0.9127 - val_accuracy: 0.6523\n",
      "Epoch 6/80\n",
      "3961/3961 [==============================] - 112s 28ms/step - loss: 0.9211 - accuracy: 0.6354 - val_loss: 0.8296 - val_accuracy: 0.6842\n",
      "Epoch 7/80\n",
      "3961/3961 [==============================] - 113s 28ms/step - loss: 0.9029 - accuracy: 0.6412 - val_loss: 0.8257 - val_accuracy: 0.6883\n",
      "Epoch 8/80\n",
      "3961/3961 [==============================] - 112s 28ms/step - loss: 0.8905 - accuracy: 0.6497 - val_loss: 0.9304 - val_accuracy: 0.6391\n",
      "Epoch 9/80\n",
      "3961/3961 [==============================] - 113s 28ms/step - loss: 0.8607 - accuracy: 0.6544 - val_loss: 0.8336 - val_accuracy: 0.6735\n",
      "Epoch 10/80\n",
      "3961/3961 [==============================] - 113s 29ms/step - loss: 0.8567 - accuracy: 0.6589 - val_loss: 0.8971 - val_accuracy: 0.6439\n",
      "Epoch 11/80\n",
      "3961/3961 [==============================] - 113s 28ms/step - loss: 0.8528 - accuracy: 0.6592 - val_loss: 0.8668 - val_accuracy: 0.6680\n",
      "Epoch 12/80\n",
      "3961/3961 [==============================] - 113s 29ms/step - loss: 0.8413 - accuracy: 0.6604 - val_loss: 0.8224 - val_accuracy: 0.6795\n",
      "Epoch 13/80\n",
      "3961/3961 [==============================] - 113s 29ms/step - loss: 0.8298 - accuracy: 0.6629 - val_loss: 0.8593 - val_accuracy: 0.6653\n",
      "Epoch 14/80\n",
      "3961/3961 [==============================] - 112s 28ms/step - loss: 0.8235 - accuracy: 0.6676 - val_loss: 0.8004 - val_accuracy: 0.6969\n",
      "Epoch 15/80\n",
      "3961/3961 [==============================] - 112s 28ms/step - loss: 0.8156 - accuracy: 0.6672 - val_loss: 0.7803 - val_accuracy: 0.7089\n",
      "Epoch 16/80\n",
      "3961/3961 [==============================] - 114s 29ms/step - loss: 0.8057 - accuracy: 0.6698 - val_loss: 0.8500 - val_accuracy: 0.6596\n",
      "Epoch 17/80\n",
      "3961/3961 [==============================] - 112s 28ms/step - loss: 0.8055 - accuracy: 0.6753 - val_loss: 0.8157 - val_accuracy: 0.6855\n",
      "Epoch 18/80\n",
      "3961/3961 [==============================] - 111s 28ms/step - loss: 0.8069 - accuracy: 0.6711 - val_loss: 0.8224 - val_accuracy: 0.6720\n",
      "Epoch 19/80\n",
      "3961/3961 [==============================] - 112s 28ms/step - loss: 0.7928 - accuracy: 0.6709 - val_loss: 0.7436 - val_accuracy: 0.7158\n",
      "Epoch 20/80\n",
      "3961/3961 [==============================] - 113s 28ms/step - loss: 0.7968 - accuracy: 0.6775 - val_loss: 0.8635 - val_accuracy: 0.6474\n",
      "Epoch 21/80\n",
      "3961/3961 [==============================] - 112s 28ms/step - loss: 0.7899 - accuracy: 0.6777 - val_loss: 0.7939 - val_accuracy: 0.6923\n",
      "Epoch 22/80\n",
      "3961/3961 [==============================] - 114s 29ms/step - loss: 0.7882 - accuracy: 0.6728 - val_loss: 0.7756 - val_accuracy: 0.6946\n",
      "Epoch 23/80\n",
      "3961/3961 [==============================] - 112s 28ms/step - loss: 0.7875 - accuracy: 0.6755 - val_loss: 0.7823 - val_accuracy: 0.6988\n",
      "Epoch 24/80\n",
      "3961/3961 [==============================] - 112s 28ms/step - loss: 0.7780 - accuracy: 0.6775 - val_loss: 0.8669 - val_accuracy: 0.6540\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-02 10:15:56.937831: I tensorflow/core/tpu/graph_rewrite/encapsulate_tpu_computations_pass.cc:237] Subgraph fingerprint:13210891179888482995\n",
      "2022-12-02 10:15:57.068978: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:954] model_pruner failed: INVALID_ARGUMENT: Graph does not contain terminal node AssignAddVariableOp.\n",
      "2022-12-02 10:15:57.251749: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:954] model_pruner failed: INVALID_ARGUMENT: Graph does not contain terminal node AssignAddVariableOp.\n",
      "2022-12-02 10:15:57.546818: I tensorflow/core/tpu/kernels/tpu_compilation_cache_interface.cc:437] TPU host compilation cache miss: cache_key(8550665179216433261), session_name()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 6s 6s/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-02 10:16:00.890102: I tensorflow/core/tpu/kernels/tpu_compile_op_common.cc:210] Compilation of 8550665179216433261 with session name  took 3.343199053s and succeeded\n",
      "2022-12-02 10:16:00.903880: I tensorflow/core/tpu/kernels/tpu_compilation_cache_interface.cc:470] TPU host compilation cache: compilation complete for cache_key(8550665179216433261), session_name(), subgraph_key(std::string(property.function_name) = \"cluster_predict_function_13210891179888482995\", property.function_library_fingerprint = 715824619521001631, property.mlir_module_fingerprint = 0, property.num_replicas = 8, topology.chip_bounds().x = 2, topology.chip_bounds().y = 2, topology.chip_bounds().z = 1, topology.wrap().x = false, topology.wrap().y = false, topology.wrap().z = false, topology.MissingChipCount() = 0, std::string(property.shapes_prefix) = \"2,240,240,4,;\", property.guaranteed_constants_size = 0, embedding_partitions_fingerprint = \"1688352644216761960\")\n",
      "2022-12-02 10:16:00.903928: I tensorflow/core/tpu/kernels/tpu_compilation_cache_interface.cc:536] After adding entry for key 8550665179216433261 with session_name  cache is 3 entries (53131146 bytes),  marked for eviction 0 entries (0 bytes).\n",
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 53). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpcch78mlo/model/data/model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpcch78mlo/model/data/model/assets\n",
      "2022/12/02 10:16:45 WARNING mlflow.utils.autologging_utils: Encountered unexpected error during tensorflow autologging: Unable to locate credentials\n",
      "2022-12-02 10:16:46.815421: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Found an unshardable source dataset: name: \"TensorSliceDataset/_1\"\n",
      "op: \"TensorSliceDataset\"\n",
      "input: \"Placeholder/_0\"\n",
      "attr {\n",
      "  key: \"Toutput_types\"\n",
      "  value {\n",
      "    list {\n",
      "      type: DT_STRING\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"_cardinality\"\n",
      "  value {\n",
      "    i: 63370\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"is_files\"\n",
      "  value {\n",
      "    b: false\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"metadata\"\n",
      "  value {\n",
      "    s: \"\\n\\024TensorSliceDataset:0\"\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_shapes\"\n",
      "  value {\n",
      "    list {\n",
      "      shape {\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"replicate_on_split\"\n",
      "  value {\n",
      "    b: false\n",
      "  }\n",
      "}\n",
      "experimental_type {\n",
      "  type_id: TFT_PRODUCT\n",
      "  args {\n",
      "    type_id: TFT_DATASET\n",
      "    args {\n",
      "      type_id: TFT_PRODUCT\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_STRING\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n",
      "2022/12/02 10:16:47 WARNING mlflow.utils.autologging_utils: Encountered unexpected error during autologging: Unable to locate credentials\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/123\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-02 10:16:57.773169: I tensorflow/core/tpu/graph_rewrite/encapsulate_tpu_computations_pass.cc:237] Subgraph fingerprint:9245565475796407030\n",
      "2022-12-02 10:16:58.286601: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:954] model_pruner failed: INVALID_ARGUMENT: Graph does not contain terminal node Adam/Adam/AssignAddVariableOp.\n",
      "2022-12-02 10:16:58.715780: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:954] model_pruner failed: INVALID_ARGUMENT: Graph does not contain terminal node Adam/Adam/AssignAddVariableOp.\n",
      "2022-12-02 10:16:59.432313: I tensorflow/core/tpu/kernels/tpu_compilation_cache_interface.cc:437] TPU host compilation cache miss: cache_key(18126227813739970558), session_name()\n",
      "2022-12-02 10:17:08.610070: I tensorflow/core/tpu/kernels/tpu_compile_op_common.cc:210] Compilation of 18126227813739970558 with session name  took 9.177672259s and succeeded\n",
      "2022-12-02 10:17:08.649734: I tensorflow/core/tpu/kernels/tpu_compilation_cache_interface.cc:470] TPU host compilation cache: compilation complete for cache_key(18126227813739970558), session_name(), subgraph_key(std::string(property.function_name) = \"cluster_train_function_9245565475796407030\", property.function_library_fingerprint = 7166629767949609860, property.mlir_module_fingerprint = 0, property.num_replicas = 8, topology.chip_bounds().x = 2, topology.chip_bounds().y = 2, topology.chip_bounds().z = 1, topology.wrap().x = false, topology.wrap().y = false, topology.wrap().z = false, topology.MissingChipCount() = 0, std::string(property.shapes_prefix) = \"2,240,240,4,;2,;2,;\", property.guaranteed_constants_size = 0, embedding_partitions_fingerprint = \"1688352644216761960\")\n",
      "2022-12-02 10:17:08.649817: I tensorflow/core/tpu/kernels/tpu_compilation_cache_interface.cc:536] After adding entry for key 18126227813739970558 with session_name  cache is 4 entries (93185601 bytes),  marked for eviction 0 entries (0 bytes).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   5/3961 [..............................] - ETA: 2:37 - loss: 0.5438 - accuracy: 0.7375  WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0336s vs `on_train_batch_end` time: 0.0445s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0336s vs `on_train_batch_end` time: 0.0445s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3960/3961 [============================>.] - ETA: 0s - loss: 0.9279 - accuracy: 0.6378"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-02 10:18:59.632680: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Found an unshardable source dataset: name: \"TensorSliceDataset/_1\"\n",
      "op: \"TensorSliceDataset\"\n",
      "input: \"Placeholder/_0\"\n",
      "attr {\n",
      "  key: \"Toutput_types\"\n",
      "  value {\n",
      "    list {\n",
      "      type: DT_STRING\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"_cardinality\"\n",
      "  value {\n",
      "    i: 15842\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"is_files\"\n",
      "  value {\n",
      "    b: false\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"metadata\"\n",
      "  value {\n",
      "    s: \"\\n\\024TensorSliceDataset:7\"\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_shapes\"\n",
      "  value {\n",
      "    list {\n",
      "      shape {\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"replicate_on_split\"\n",
      "  value {\n",
      "    b: false\n",
      "  }\n",
      "}\n",
      "experimental_type {\n",
      "  type_id: TFT_PRODUCT\n",
      "  args {\n",
      "    type_id: TFT_DATASET\n",
      "    args {\n",
      "      type_id: TFT_PRODUCT\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_STRING\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n",
      "2022-12-02 10:19:02.129333: I tensorflow/core/tpu/graph_rewrite/encapsulate_tpu_computations_pass.cc:237] Subgraph fingerprint:7162480873429290547\n",
      "2022-12-02 10:19:02.277401: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:954] model_pruner failed: INVALID_ARGUMENT: Graph does not contain terminal node AssignAddVariableOp.\n",
      "2022-12-02 10:19:02.496131: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:954] model_pruner failed: INVALID_ARGUMENT: Graph does not contain terminal node AssignAddVariableOp.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3961/3961 [==============================] - 150s 33ms/step - loss: 0.9278 - accuracy: 0.6378 - val_loss: 0.9159 - val_accuracy: 0.6276\n",
      "Epoch 26/123\n",
      "3961/3961 [==============================] - 129s 32ms/step - loss: 0.8109 - accuracy: 0.6765 - val_loss: 0.7367 - val_accuracy: 0.7121\n",
      "Epoch 27/123\n",
      "3961/3961 [==============================] - 128s 32ms/step - loss: 0.7170 - accuracy: 0.7090 - val_loss: 0.6596 - val_accuracy: 0.7536\n",
      "Epoch 28/123\n",
      "3961/3961 [==============================] - 128s 32ms/step - loss: 0.6649 - accuracy: 0.7232 - val_loss: 0.6227 - val_accuracy: 0.7442\n",
      "Epoch 29/123\n",
      "3961/3961 [==============================] - 127s 32ms/step - loss: 0.5994 - accuracy: 0.7449 - val_loss: 0.6087 - val_accuracy: 0.7636\n",
      "Epoch 30/123\n",
      "3961/3961 [==============================] - 128s 32ms/step - loss: 0.5587 - accuracy: 0.7586 - val_loss: 0.7985 - val_accuracy: 0.6708\n",
      "Epoch 31/123\n",
      "3961/3961 [==============================] - 126s 32ms/step - loss: 0.5189 - accuracy: 0.7689 - val_loss: 0.5714 - val_accuracy: 0.7676\n",
      "Epoch 32/123\n",
      "3961/3961 [==============================] - 127s 32ms/step - loss: 0.4762 - accuracy: 0.7827 - val_loss: 0.5851 - val_accuracy: 0.7870\n",
      "Epoch 33/123\n",
      "3961/3961 [==============================] - 126s 32ms/step - loss: 0.4391 - accuracy: 0.7943 - val_loss: 0.5202 - val_accuracy: 0.7978\n",
      "Epoch 34/123\n",
      "3961/3961 [==============================] - 127s 32ms/step - loss: 0.4040 - accuracy: 0.8045 - val_loss: 0.6842 - val_accuracy: 0.7313\n",
      "Epoch 35/123\n",
      "3961/3961 [==============================] - 126s 32ms/step - loss: 0.3843 - accuracy: 0.8108 - val_loss: 0.4583 - val_accuracy: 0.8219\n",
      "Epoch 36/123\n",
      "3961/3961 [==============================] - 126s 32ms/step - loss: 0.3432 - accuracy: 0.8256 - val_loss: 0.4368 - val_accuracy: 0.8294\n",
      "Epoch 37/123\n",
      "3961/3961 [==============================] - 126s 32ms/step - loss: 0.3192 - accuracy: 0.8319 - val_loss: 0.4600 - val_accuracy: 0.8224\n",
      "Epoch 38/123\n",
      "3961/3961 [==============================] - 128s 32ms/step - loss: 0.2909 - accuracy: 0.8425 - val_loss: 0.5323 - val_accuracy: 0.7950\n",
      "Epoch 39/123\n",
      "3961/3961 [==============================] - 127s 32ms/step - loss: 0.2744 - accuracy: 0.8522 - val_loss: 0.3799 - val_accuracy: 0.8550\n",
      "Epoch 40/123\n",
      "3961/3961 [==============================] - 127s 32ms/step - loss: 0.2559 - accuracy: 0.8582 - val_loss: 0.3697 - val_accuracy: 0.8587\n",
      "Epoch 41/123\n",
      "3961/3961 [==============================] - 126s 32ms/step - loss: 0.2318 - accuracy: 0.8700 - val_loss: 0.3452 - val_accuracy: 0.8723\n",
      "Epoch 42/123\n",
      "3961/3961 [==============================] - 125s 32ms/step - loss: 0.2111 - accuracy: 0.8780 - val_loss: 0.3896 - val_accuracy: 0.8511\n",
      "Epoch 43/123\n",
      "3961/3961 [==============================] - 126s 32ms/step - loss: 0.1932 - accuracy: 0.8854 - val_loss: 0.3462 - val_accuracy: 0.8755\n",
      "Epoch 44/123\n",
      "3961/3961 [==============================] - 127s 32ms/step - loss: 0.1852 - accuracy: 0.8917 - val_loss: 0.3352 - val_accuracy: 0.8805\n",
      "Epoch 45/123\n",
      "3961/3961 [==============================] - 126s 32ms/step - loss: 0.1681 - accuracy: 0.8991 - val_loss: 0.3664 - val_accuracy: 0.8666\n",
      "Epoch 46/123\n",
      "3961/3961 [==============================] - 126s 32ms/step - loss: 0.1677 - accuracy: 0.9018 - val_loss: 0.3294 - val_accuracy: 0.8820\n",
      "Epoch 47/123\n",
      "3961/3961 [==============================] - 125s 32ms/step - loss: 0.1508 - accuracy: 0.9099 - val_loss: 0.3393 - val_accuracy: 0.8737\n",
      "Epoch 48/123\n",
      "3961/3961 [==============================] - 126s 32ms/step - loss: 0.1514 - accuracy: 0.9099 - val_loss: 0.3211 - val_accuracy: 0.8874\n",
      "Epoch 49/123\n",
      "3961/3961 [==============================] - 128s 32ms/step - loss: 0.1334 - accuracy: 0.9189 - val_loss: 0.3470 - val_accuracy: 0.8758\n",
      "Epoch 50/123\n",
      "3961/3961 [==============================] - 126s 32ms/step - loss: 0.1355 - accuracy: 0.9208 - val_loss: 0.3329 - val_accuracy: 0.8868\n",
      "Epoch 51/123\n",
      "3961/3961 [==============================] - 127s 32ms/step - loss: 0.1257 - accuracy: 0.9261 - val_loss: 0.4010 - val_accuracy: 0.8502\n",
      "Epoch 52/123\n",
      "3961/3961 [==============================] - 126s 32ms/step - loss: 0.1077 - accuracy: 0.9325 - val_loss: 0.3470 - val_accuracy: 0.8809\n",
      "Epoch 53/123\n",
      "3961/3961 [==============================] - 125s 32ms/step - loss: 0.1022 - accuracy: 0.9365 - val_loss: 0.3860 - val_accuracy: 0.8789\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-02 11:18:41.976362: I tensorflow/core/tpu/graph_rewrite/encapsulate_tpu_computations_pass.cc:237] Subgraph fingerprint:13210891179888482995\n",
      "2022-12-02 11:18:42.103739: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:954] model_pruner failed: INVALID_ARGUMENT: Graph does not contain terminal node AssignAddVariableOp.\n",
      "2022-12-02 11:18:42.286476: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:954] model_pruner failed: INVALID_ARGUMENT: Graph does not contain terminal node AssignAddVariableOp.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 2s 2s/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 53). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmp_n2273vt/model/data/model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmp_n2273vt/model/data/model/assets\n",
      "2022/12/02 11:19:21 WARNING mlflow.utils.autologging_utils: Encountered unexpected error during tensorflow autologging: Unable to locate credentials\n",
      "2022-12-02 11:19:23.567855: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Found an unshardable source dataset: name: \"TensorSliceDataset/_1\"\n",
      "op: \"TensorSliceDataset\"\n",
      "input: \"Placeholder/_0\"\n",
      "attr {\n",
      "  key: \"Toutput_types\"\n",
      "  value {\n",
      "    list {\n",
      "      type: DT_STRING\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"_cardinality\"\n",
      "  value {\n",
      "    i: 63370\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"is_files\"\n",
      "  value {\n",
      "    b: false\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"metadata\"\n",
      "  value {\n",
      "    s: \"\\n\\024TensorSliceDataset:0\"\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_shapes\"\n",
      "  value {\n",
      "    list {\n",
      "      shape {\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"replicate_on_split\"\n",
      "  value {\n",
      "    b: false\n",
      "  }\n",
      "}\n",
      "experimental_type {\n",
      "  type_id: TFT_PRODUCT\n",
      "  args {\n",
      "    type_id: TFT_DATASET\n",
      "    args {\n",
      "      type_id: TFT_PRODUCT\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_STRING\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n",
      "2022/12/02 11:19:24 WARNING mlflow.utils.autologging_utils: Encountered unexpected error during autologging: Unable to locate credentials\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 54/152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-02 11:19:42.699131: I tensorflow/core/tpu/graph_rewrite/encapsulate_tpu_computations_pass.cc:237] Subgraph fingerprint:10506435885300261483\n",
      "2022-12-02 11:19:43.632768: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:954] model_pruner failed: INVALID_ARGUMENT: Graph does not contain terminal node Adam/Adam/AssignAddVariableOp.\n",
      "2022-12-02 11:19:44.214182: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:954] model_pruner failed: INVALID_ARGUMENT: Graph does not contain terminal node Adam/Adam/AssignAddVariableOp.\n",
      "2022-12-02 11:19:45.360044: I tensorflow/core/tpu/kernels/tpu_compilation_cache_interface.cc:437] TPU host compilation cache miss: cache_key(1042959909242842276), session_name()\n",
      "2022-12-02 11:20:00.041748: I tensorflow/core/tpu/kernels/tpu_compile_op_common.cc:210] Compilation of 1042959909242842276 with session name  took 14.681586981s and succeeded\n",
      "2022-12-02 11:20:00.085391: I tensorflow/core/tpu/kernels/tpu_compilation_cache_interface.cc:470] TPU host compilation cache: compilation complete for cache_key(1042959909242842276), session_name(), subgraph_key(std::string(property.function_name) = \"cluster_train_function_10506435885300261483\", property.function_library_fingerprint = 14550379847358377251, property.mlir_module_fingerprint = 0, property.num_replicas = 8, topology.chip_bounds().x = 2, topology.chip_bounds().y = 2, topology.chip_bounds().z = 1, topology.wrap().x = false, topology.wrap().y = false, topology.wrap().z = false, topology.MissingChipCount() = 0, std::string(property.shapes_prefix) = \"2,240,240,4,;2,;2,;\", property.guaranteed_constants_size = 0, embedding_partitions_fingerprint = \"1688352644216761960\")\n",
      "2022-12-02 11:20:00.085487: I tensorflow/core/tpu/kernels/tpu_compilation_cache_interface.cc:536] After adding entry for key 1042959909242842276 with session_name  cache is 5 entries (156952514 bytes),  marked for eviction 0 entries (0 bytes).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   6/3961 [..............................] - ETA: 2:52 - loss: 0.2267 - accuracy: 0.8958WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0393s vs `on_train_batch_end` time: 0.0712s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0393s vs `on_train_batch_end` time: 0.0712s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3961/3961 [==============================] - ETA: 0s - loss: 0.2222 - accuracy: 0.9021"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-02 11:22:22.957223: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Found an unshardable source dataset: name: \"TensorSliceDataset/_1\"\n",
      "op: \"TensorSliceDataset\"\n",
      "input: \"Placeholder/_0\"\n",
      "attr {\n",
      "  key: \"Toutput_types\"\n",
      "  value {\n",
      "    list {\n",
      "      type: DT_STRING\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"_cardinality\"\n",
      "  value {\n",
      "    i: 15842\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"is_files\"\n",
      "  value {\n",
      "    b: false\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"metadata\"\n",
      "  value {\n",
      "    s: \"\\n\\024TensorSliceDataset:7\"\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_shapes\"\n",
      "  value {\n",
      "    list {\n",
      "      shape {\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"replicate_on_split\"\n",
      "  value {\n",
      "    b: false\n",
      "  }\n",
      "}\n",
      "experimental_type {\n",
      "  type_id: TFT_PRODUCT\n",
      "  args {\n",
      "    type_id: TFT_DATASET\n",
      "    args {\n",
      "      type_id: TFT_PRODUCT\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_STRING\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n",
      "2022-12-02 11:22:25.245711: I tensorflow/core/tpu/graph_rewrite/encapsulate_tpu_computations_pass.cc:237] Subgraph fingerprint:7162480873429290547\n",
      "2022-12-02 11:22:25.389376: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:954] model_pruner failed: INVALID_ARGUMENT: Graph does not contain terminal node AssignAddVariableOp.\n",
      "2022-12-02 11:22:25.603364: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:954] model_pruner failed: INVALID_ARGUMENT: Graph does not contain terminal node AssignAddVariableOp.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 54: val_loss improved from inf to 0.31785, saving model to ../model_checkpoints/mobilenetv2_bs16_pat5_del0.001_dr0.25_lr0.0001_save-2022-12-02-09:29:15-054-0.3179.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 53). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../model_checkpoints/mobilenetv2_bs16_pat5_del0.001_dr0.25_lr0.0001_save-2022-12-02-09:29:15-054-0.3179.ckpt/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../model_checkpoints/mobilenetv2_bs16_pat5_del0.001_dr0.25_lr0.0001_save-2022-12-02-09:29:15-054-0.3179.ckpt/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3961/3961 [==============================] - 225s 48ms/step - loss: 0.2222 - accuracy: 0.9021 - val_loss: 0.3179 - val_accuracy: 0.8844\n",
      "Epoch 55/152\n",
      "3961/3961 [==============================] - ETA: 0s - loss: 0.2121 - accuracy: 0.8978\n",
      "Epoch 55: val_loss did not improve from 0.31785\n",
      "3961/3961 [==============================] - 160s 41ms/step - loss: 0.2121 - accuracy: 0.8978 - val_loss: 0.5240 - val_accuracy: 0.7843\n",
      "Epoch 56/152\n",
      "3961/3961 [==============================] - ETA: 0s - loss: 0.2315 - accuracy: 0.8893\n",
      "Epoch 56: val_loss did not improve from 0.31785\n",
      "3961/3961 [==============================] - 160s 40ms/step - loss: 0.2315 - accuracy: 0.8893 - val_loss: 0.4388 - val_accuracy: 0.8387\n",
      "Epoch 57/152\n",
      "3961/3961 [==============================] - ETA: 0s - loss: 0.1884 - accuracy: 0.9088\n",
      "Epoch 57: val_loss improved from 0.31785 to 0.31542, saving model to ../model_checkpoints/mobilenetv2_bs16_pat5_del0.001_dr0.25_lr0.0001_save-2022-12-02-09:29:15-057-0.3154.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 53). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../model_checkpoints/mobilenetv2_bs16_pat5_del0.001_dr0.25_lr0.0001_save-2022-12-02-09:29:15-057-0.3154.ckpt/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../model_checkpoints/mobilenetv2_bs16_pat5_del0.001_dr0.25_lr0.0001_save-2022-12-02-09:29:15-057-0.3154.ckpt/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3961/3961 [==============================] - 189s 48ms/step - loss: 0.1884 - accuracy: 0.9088 - val_loss: 0.3154 - val_accuracy: 0.8886\n",
      "Epoch 58/152\n",
      "3960/3961 [============================>.] - ETA: 0s - loss: 0.1646 - accuracy: 0.9136\n",
      "Epoch 58: val_loss improved from 0.31542 to 0.30773, saving model to ../model_checkpoints/mobilenetv2_bs16_pat5_del0.001_dr0.25_lr0.0001_save-2022-12-02-09:29:15-058-0.3077.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 53). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../model_checkpoints/mobilenetv2_bs16_pat5_del0.001_dr0.25_lr0.0001_save-2022-12-02-09:29:15-058-0.3077.ckpt/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../model_checkpoints/mobilenetv2_bs16_pat5_del0.001_dr0.25_lr0.0001_save-2022-12-02-09:29:15-058-0.3077.ckpt/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3961/3961 [==============================] - 189s 48ms/step - loss: 0.1646 - accuracy: 0.9136 - val_loss: 0.3077 - val_accuracy: 0.8955\n",
      "Epoch 59/152\n",
      "3961/3961 [==============================] - ETA: 0s - loss: 0.1207 - accuracy: 0.9325\n",
      "Epoch 59: val_loss did not improve from 0.30773\n",
      "3961/3961 [==============================] - 160s 40ms/step - loss: 0.1207 - accuracy: 0.9325 - val_loss: 0.3676 - val_accuracy: 0.8669\n",
      "Epoch 60/152\n",
      "3961/3961 [==============================] - ETA: 0s - loss: 0.2722 - accuracy: 0.8836\n",
      "Epoch 60: val_loss did not improve from 0.30773\n",
      "3961/3961 [==============================] - 160s 40ms/step - loss: 0.2722 - accuracy: 0.8836 - val_loss: 0.3386 - val_accuracy: 0.8758\n",
      "Epoch 61/152\n",
      "3960/3961 [============================>.] - ETA: 0s - loss: 0.1225 - accuracy: 0.9337\n",
      "Epoch 61: val_loss did not improve from 0.30773\n",
      "3961/3961 [==============================] - 159s 40ms/step - loss: 0.1226 - accuracy: 0.9337 - val_loss: 0.4010 - val_accuracy: 0.8614\n",
      "Epoch 62/152\n",
      "3961/3961 [==============================] - ETA: 0s - loss: 0.1661 - accuracy: 0.9193\n",
      "Epoch 62: val_loss improved from 0.30773 to 0.27486, saving model to ../model_checkpoints/mobilenetv2_bs16_pat5_del0.001_dr0.25_lr0.0001_save-2022-12-02-09:29:15-062-0.2749.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 53). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../model_checkpoints/mobilenetv2_bs16_pat5_del0.001_dr0.25_lr0.0001_save-2022-12-02-09:29:15-062-0.2749.ckpt/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../model_checkpoints/mobilenetv2_bs16_pat5_del0.001_dr0.25_lr0.0001_save-2022-12-02-09:29:15-062-0.2749.ckpt/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3961/3961 [==============================] - 190s 48ms/step - loss: 0.1661 - accuracy: 0.9193 - val_loss: 0.2749 - val_accuracy: 0.9123\n",
      "Epoch 63/152\n",
      "3960/3961 [============================>.] - ETA: 0s - loss: 0.1091 - accuracy: 0.9391\n",
      "Epoch 63: val_loss did not improve from 0.27486\n",
      "3961/3961 [==============================] - 160s 40ms/step - loss: 0.1091 - accuracy: 0.9391 - val_loss: 0.2810 - val_accuracy: 0.9164\n",
      "Epoch 64/152\n",
      "3960/3961 [============================>.] - ETA: 0s - loss: 0.1676 - accuracy: 0.9208\n",
      "Epoch 64: val_loss did not improve from 0.27486\n",
      "3961/3961 [==============================] - 160s 41ms/step - loss: 0.1676 - accuracy: 0.9208 - val_loss: 0.3103 - val_accuracy: 0.9039\n",
      "Epoch 65/152\n",
      "3961/3961 [==============================] - ETA: 0s - loss: 0.1108 - accuracy: 0.9408\n",
      "Epoch 65: val_loss did not improve from 0.27486\n",
      "3961/3961 [==============================] - 159s 40ms/step - loss: 0.1108 - accuracy: 0.9408 - val_loss: 0.3226 - val_accuracy: 0.9012\n",
      "Epoch 66/152\n",
      "3961/3961 [==============================] - ETA: 0s - loss: 0.1176 - accuracy: 0.9390\n",
      "Epoch 66: val_loss did not improve from 0.27486\n",
      "3961/3961 [==============================] - 158s 40ms/step - loss: 0.1176 - accuracy: 0.9390 - val_loss: 0.3095 - val_accuracy: 0.8999\n",
      "Epoch 67/152\n",
      "3961/3961 [==============================] - ETA: 0s - loss: 0.1173 - accuracy: 0.9409\n",
      "Epoch 67: val_loss did not improve from 0.27486\n",
      "3961/3961 [==============================] - 157s 40ms/step - loss: 0.1173 - accuracy: 0.9409 - val_loss: 0.3272 - val_accuracy: 0.8911\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-02 11:59:20.172417: I tensorflow/core/tpu/graph_rewrite/encapsulate_tpu_computations_pass.cc:237] Subgraph fingerprint:13210891179888482995\n",
      "2022-12-02 11:59:20.298765: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:954] model_pruner failed: INVALID_ARGUMENT: Graph does not contain terminal node AssignAddVariableOp.\n",
      "2022-12-02 11:59:20.480190: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:954] model_pruner failed: INVALID_ARGUMENT: Graph does not contain terminal node AssignAddVariableOp.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 2s 2s/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 53). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmp2cyqo5bn/model/data/model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmp2cyqo5bn/model/data/model/assets\n",
      "2022/12/02 12:00:04 WARNING mlflow.utils.autologging_utils: Encountered unexpected error during tensorflow autologging: Unable to locate credentials\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "with mlflow.start_run(\n",
    "    run_name=parent_run_name,\n",
    "    tags={\n",
    "        'dataset': dataset,\n",
    "    },\n",
    "    description=run_description,\n",
    "):\n",
    "\n",
    "    img_height = 240\n",
    "    img_width = 240\n",
    "    data_dir = os.path.join('..','data','UPENN-GBM','slice_classification_common_stratify_healthysegmented','train')\n",
    "\n",
    "    train_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "        data_dir,\n",
    "        validation_split=0.2,\n",
    "        subset=\"training\",\n",
    "        color_mode=\"rgba\",\n",
    "        seed=RSEED,\n",
    "        image_size=(img_height, img_width),\n",
    "        batch_size=batch_size)\n",
    "\n",
    "    val_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "        data_dir,\n",
    "        validation_split=0.2,\n",
    "        subset=\"validation\",\n",
    "        color_mode=\"rgba\",\n",
    "        seed=RSEED,\n",
    "        image_size=(img_height, img_width),\n",
    "        batch_size=batch_size)\n",
    "\n",
    "    \n",
    "    class_names = train_ds.class_names\n",
    "\n",
    "    # Calculate class weights for weighting accuracy\n",
    "    ds_classes = []\n",
    "    for _, batch_classes in train_ds:\n",
    "        ds_classes.append(batch_classes.numpy())\n",
    "\n",
    "    ds_classes = np.concatenate(ds_classes)\n",
    "\n",
    "    class_weight = compute_class_weight(\n",
    "        class_weight = 'balanced',\n",
    "        classes = np.unique(ds_classes),\n",
    "        y=ds_classes\n",
    "    )\n",
    "\n",
    "    class_weight = dict(zip(np.unique(ds_classes), class_weight))\n",
    "\n",
    "    AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "    train_ds = train_ds.cache().shuffle(1000).prefetch(buffer_size=AUTOTUNE)\n",
    "    val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "\n",
    "    num_classes = len(class_names)\n",
    "    \n",
    "    margin = 8\n",
    "    scaled_height = img_height - 2*margin\n",
    "    scaled_width = img_width - 2*margin\n",
    "\n",
    "    # Build layers for model with fixed base\n",
    "    with strategy.scope():\n",
    "        crop_layer = tf.keras.layers.Cropping2D(margin)\n",
    "        rescale_initial = tf.keras.layers.Rescaling(1./127.5, offset=-1)\n",
    "        conv_4to3_channel = tf.keras.layers.Conv2D(3,1,padding='same',activation='tanh')\n",
    "        base_model = tf.keras.applications.MobileNetV2(\n",
    "            input_shape=(scaled_width,scaled_height,3),\n",
    "            include_top=False,\n",
    "            weights='imagenet'\n",
    "        )\n",
    "        global_average_layer = tf.keras.layers.GlobalAveragePooling2D()\n",
    "        prediction_layer = tf.keras.layers.Dense(num_classes)\n",
    "\n",
    "        base_model.trainable = False\n",
    "        \n",
    "        inputs = tf.keras.Input(shape=(img_width, img_height, 4))\n",
    "        x = crop_layer(inputs)\n",
    "        x = rescale_initial(x)\n",
    "        x = conv_4to3_channel(x)\n",
    "        x = base_model(x, training=False)\n",
    "        x = global_average_layer(x)\n",
    "        x = tf.keras.layers.Dropout(dropout_rate)(x)\n",
    "        outputs = prediction_layer(x)\n",
    "    \n",
    "        earlystopping = tf.keras.callbacks.EarlyStopping(\n",
    "            monitor='val_loss',\n",
    "            patience=patience,\n",
    "            min_delta=min_delta,\n",
    "            )\n",
    "        \n",
    "        model = tf.keras.Model(inputs, outputs)\n",
    "        model.compile(\n",
    "            optimizer=tf.keras.optimizers.Adam(learning_rate=initial_learning_rate,),\n",
    "            loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "            metrics=['accuracy'],\n",
    "        )\n",
    "\n",
    "        \n",
    "    # Initial fit of classification and 4 to 3 channel layers\n",
    "    with mlflow.start_run(\n",
    "        run_name=f'fixed_{run_name_params}',\n",
    "        tags={'dataset': dataset},\n",
    "        nested=True\n",
    "    ):\n",
    "        mlflow.tensorflow.autolog()\n",
    "        mlflow.log_param('ds_batch_size', batch_size)\n",
    "        mlflow.log_param('ds_validation_batch_size', batch_size)\n",
    "\n",
    "\n",
    "        fixed_base_epochs=80\n",
    "        history_fixed_base = model.fit(\n",
    "            train_ds,\n",
    "            validation_data=val_ds,\n",
    "            epochs=fixed_base_epochs,\n",
    "            class_weight=class_weight,\n",
    "            callbacks=[earlystopping],\n",
    "        )\n",
    "\n",
    "    # Relax top layers of base model\n",
    "    base_model.trainable = True\n",
    "    fix_below_layer = 100\n",
    "    for layer in base_model.layers[:fix_below_layer]:\n",
    "        layer.trainable = False\n",
    "    with strategy.scope():\n",
    "        model.compile(\n",
    "            optimizer=tf.keras.optimizers.Adam(learning_rate=initial_learning_rate/10.0),\n",
    "                loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "                metrics=['accuracy'],\n",
    "        )\n",
    "\n",
    "    with mlflow.start_run(\n",
    "        run_name=f'partial_{run_name_params}',\n",
    "        tags={'dataset': dataset},\n",
    "        nested=True\n",
    "    ):\n",
    "        mlflow.tensorflow.autolog()\n",
    "        mlflow.log_param('ds_batch_size', batch_size)\n",
    "        mlflow.log_param('ds_validation_batch_size', batch_size)\n",
    "\n",
    "        partial_relax_epochs=history_fixed_base.epoch[-1] + 100 \n",
    "        history_partial_relax = model.fit(\n",
    "            train_ds,\n",
    "            validation_data=val_ds,\n",
    "            epochs=partial_relax_epochs,\n",
    "            initial_epoch=history_fixed_base.epoch[-1]+1,\n",
    "            class_weight=class_weight,\n",
    "            callbacks=[earlystopping],\n",
    "        )\n",
    "\n",
    "    # Fully relax model\n",
    "    model.trainable = True\n",
    "\n",
    "    with strategy.scope():\n",
    "        model.compile(\n",
    "            optimizer=tf.keras.optimizers.Adam(learning_rate=initial_learning_rate/10.0),\n",
    "            loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "            metrics=['accuracy'],\n",
    "        )\n",
    "\n",
    "    with mlflow.start_run(\n",
    "        run_name=f'relax_{run_name_params}',\n",
    "        tags={'dataset': dataset},\n",
    "        nested=True\n",
    "    ):\n",
    "        mlflow.tensorflow.autolog()\n",
    "        mlflow.log_param('ds_batch_size', batch_size)\n",
    "        mlflow.log_param('ds_validation_batch_size', batch_size)\n",
    "\n",
    "        # create checkpoint\n",
    "        checkpoint_path = os.path.join(\n",
    "            MODEL_CHECKPOINTS_DIR,\n",
    "            parent_run_name + start_time + \"-{epoch:03d}-{val_loss:.4f}.ckpt\"\n",
    "        )\n",
    "        ckpt_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "            filepath=checkpoint_path, \n",
    "            verbose=1, \n",
    "            save_weights_only=False,\n",
    "            save_freq='epoch',\n",
    "            monitor='val_loss',\n",
    "            mode='min',\n",
    "            save_best_only=True,\n",
    "        ) \n",
    "\n",
    "        full_relax_epochs=history_partial_relax.epoch[-1] + 100\n",
    "        history_full_relax = model.fit(\n",
    "            train_ds,\n",
    "            validation_data=val_ds,\n",
    "            epochs=full_relax_epochs,\n",
    "            initial_epoch=history_partial_relax.epoch[-1]+1,\n",
    "            class_weight=class_weight,\n",
    "            callbacks=[earlystopping, ckpt_callback],\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 9605 files belonging to 6 classes.\n"
     ]
    }
   ],
   "source": [
    "test_data_dir = os.path.join('..','data','UPENN-GBM','slice_classification_common_stratify_healthysegmented','test')\n",
    "\n",
    "test_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "    test_data_dir,\n",
    "    color_mode=\"rgba\",\n",
    "    seed=RSEED,\n",
    "    shuffle=False,\n",
    "    image_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-02 12:00:06.217306: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Found an unshardable source dataset: name: \"TensorSliceDataset/_1\"\n",
      "op: \"TensorSliceDataset\"\n",
      "input: \"Placeholder/_0\"\n",
      "attr {\n",
      "  key: \"Toutput_types\"\n",
      "  value {\n",
      "    list {\n",
      "      type: DT_STRING\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"_cardinality\"\n",
      "  value {\n",
      "    i: 15842\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"is_files\"\n",
      "  value {\n",
      "    b: false\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"metadata\"\n",
      "  value {\n",
      "    s: \"\\n\\024TensorSliceDataset:7\"\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_shapes\"\n",
      "  value {\n",
      "    list {\n",
      "      shape {\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"replicate_on_split\"\n",
      "  value {\n",
      "    b: false\n",
      "  }\n",
      "}\n",
      "experimental_type {\n",
      "  type_id: TFT_PRODUCT\n",
      "  args {\n",
      "    type_id: TFT_DATASET\n",
      "    args {\n",
      "      type_id: TFT_PRODUCT\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_STRING\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n",
      "2022-12-02 12:00:08.622095: I tensorflow/core/tpu/graph_rewrite/encapsulate_tpu_computations_pass.cc:237] Subgraph fingerprint:10454398515663473433\n",
      "2022-12-02 12:00:08.774619: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:954] model_pruner failed: INVALID_ARGUMENT: Graph does not contain terminal node AssignAddVariableOp.\n",
      "2022-12-02 12:00:09.009971: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:954] model_pruner failed: INVALID_ARGUMENT: Graph does not contain terminal node AssignAddVariableOp.\n",
      "2022-12-02 12:00:09.380124: I tensorflow/core/tpu/kernels/tpu_compilation_cache_interface.cc:437] TPU host compilation cache miss: cache_key(4858963082524620730), session_name()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  6/991 [..............................] - ETA: 24s   "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-02 12:00:13.135835: I tensorflow/core/tpu/kernels/tpu_compile_op_common.cc:210] Compilation of 4858963082524620730 with session name  took 3.755608825s and succeeded\n",
      "2022-12-02 12:00:13.148943: I tensorflow/core/tpu/kernels/tpu_compilation_cache_interface.cc:470] TPU host compilation cache: compilation complete for cache_key(4858963082524620730), session_name(), subgraph_key(std::string(property.function_name) = \"cluster_predict_function_10454398515663473433\", property.function_library_fingerprint = 6712631106669902044, property.mlir_module_fingerprint = 0, property.num_replicas = 8, topology.chip_bounds().x = 2, topology.chip_bounds().y = 2, topology.chip_bounds().z = 1, topology.wrap().x = false, topology.wrap().y = false, topology.wrap().z = false, topology.MissingChipCount() = 0, std::string(property.shapes_prefix) = \"2,240,240,4,;2,;\", property.guaranteed_constants_size = 0, embedding_partitions_fingerprint = \"1688352644216761960\")\n",
      "2022-12-02 12:00:13.149015: I tensorflow/core/tpu/kernels/tpu_compilation_cache_interface.cc:536] After adding entry for key 4858963082524620730 with session_name  cache is 6 entries (169299793 bytes),  marked for eviction 0 entries (0 bytes).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "991/991 [==============================] - 26s 19ms/step\n"
     ]
    }
   ],
   "source": [
    "val_pred = model.predict(val_ds)\n",
    "val_prob = tf.nn.softmax(val_pred)\n",
    "val_class_pred = [np.argmax(x) for x in val_prob]\n",
    "val_base = [ 0 for x in val_class_pred ]\n",
    "\n",
    "val_true_class = []\n",
    "for _, classes in val_ds:\n",
    "    val_true_class += list(classes)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-02 12:00:40.167474: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Found an unshardable source dataset: name: \"TensorSliceDataset/_1\"\n",
      "op: \"TensorSliceDataset\"\n",
      "input: \"Placeholder/_0\"\n",
      "attr {\n",
      "  key: \"Toutput_types\"\n",
      "  value {\n",
      "    list {\n",
      "      type: DT_STRING\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"_cardinality\"\n",
      "  value {\n",
      "    i: 9605\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"is_files\"\n",
      "  value {\n",
      "    b: false\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"metadata\"\n",
      "  value {\n",
      "    s: \"\\n\\027TensorSliceDataset:5711\"\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_shapes\"\n",
      "  value {\n",
      "    list {\n",
      "      shape {\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"replicate_on_split\"\n",
      "  value {\n",
      "    b: false\n",
      "  }\n",
      "}\n",
      "experimental_type {\n",
      "  type_id: TFT_PRODUCT\n",
      "  args {\n",
      "    type_id: TFT_DATASET\n",
      "    args {\n",
      "      type_id: TFT_PRODUCT\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_STRING\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "601/601 [==============================] - 12s 19ms/step\n"
     ]
    }
   ],
   "source": [
    "test_pred = model.predict(test_ds)\n",
    "test_prob = tf.nn.softmax(test_pred)\n",
    "test_class_pred = [np.argmax(x) for x in test_prob]\n",
    "test_base = [ 0 for x in test_class_pred ]\n",
    "\n",
    "test_true_class = []\n",
    "for _, classes in test_ds:\n",
    "    test_true_class += list(classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['background',\n",
       " 'background_edema_healthy',\n",
       " 'background_edema_healthy_contrast',\n",
       " 'background_healthy',\n",
       " 'background_tumour_edema_healthy',\n",
       " 'background_tumour_edema_healthy_contrast']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      1.00      0.99       528\n",
      "           1       0.69      0.85      0.76      2499\n",
      "           2       0.32      0.58      0.42       293\n",
      "           3       0.96      0.90      0.93      8465\n",
      "           4       0.49      0.72      0.58        53\n",
      "           5       0.98      0.90      0.94      4004\n",
      "\n",
      "    accuracy                           0.89     15842\n",
      "   macro avg       0.74      0.83      0.77     15842\n",
      "weighted avg       0.91      0.89      0.90     15842\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(val_true_class, val_class_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      1.00       965\n",
      "           1       0.52      0.66      0.58      1361\n",
      "           2       0.17      0.37      0.23       166\n",
      "           3       0.91      0.86      0.88      4743\n",
      "           4       0.00      0.00      0.00        69\n",
      "           5       0.96      0.89      0.93      2301\n",
      "\n",
      "    accuracy                           0.84      9605\n",
      "   macro avg       0.60      0.63      0.60      9605\n",
      "weighted avg       0.86      0.84      0.85      9605\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(test_true_class, test_class_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 53). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../models/mobilenetv2_bs16_pat5_del0.001_dr0.25_lr0.0001_save-2022-12-02-09:29:15/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../models/mobilenetv2_bs16_pat5_del0.001_dr0.25_lr0.0001_save-2022-12-02-09:29:15/assets\n"
     ]
    }
   ],
   "source": [
    "model_file_name = os.path.join(MODELS_DIR, parent_run_name + start_time)\n",
    "model.save(model_file_name)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
